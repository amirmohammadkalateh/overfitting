{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2fAEUaILUrsR+RFhr0MJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirmohammadkalateh/overfitting/blob/main/kernel_regularizer_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqjnvlAwfW8m",
        "outputId": "7ed4163f-24f9-4dfa-b151-bce7d07f24ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regularized ANN model architecture saved to 'regularized_ann_model.png'\n",
            "\n",
            "--- Explanation of the Model and Regularization ---\n",
            "\n",
            "**Model Architecture:**\n",
            "The model is a sequential Artificial Neural Network (ANN) built using TensorFlow/Keras.\n",
            "It consists of the following layers:\n",
            "- **Input Layer:** Accepts input features of dimension 10.\n",
            "- **Hidden Layer 1:** A dense layer with 64 units and ReLU activation.\n",
            "  - **Kernel Regularization:** Both L1 and L2 regularization are applied to the weights (kernel) of this layer.\n",
            "- **Hidden Layer 2:** A dense layer with 32 units and ReLU activation.\n",
            "  - **Kernel Regularization:** Both L1 and L2 regularization are applied to the weights (kernel) of this layer.\n",
            "- **Output Layer:** A dense layer with 1 unit and sigmoid activation (for binary classification).\n",
            "\n",
            "**L1 and L2 Regularization:**\n",
            "Regularization techniques are used to prevent overfitting in machine learning models by adding a penalty term to the loss function.\n",
            "This penalty discourages the model from learning overly complex patterns from the training data.\n",
            "\n",
            "**1. L1 Regularization (Lasso):**\n",
            "- Adds a penalty equal to the absolute value of the weights multiplied by a regularization strength ($\\lambda_1$).\n",
            "- The L1 regularization term in the loss function is: $$\\lambda_1 \\sum_{i=1}^{n} |w_i|$$\n",
            "- **Effect:** Tends to drive some weights to exactly zero, leading to sparse weight matrices. This can be useful for feature selection as it effectively makes some features irrelevant.\n",
            "- In this model, the L1 regularization strength ($\\lambda_1$) is set to 0.005.\n",
            "\n",
            "**2. L2 Regularization (Ridge):**\n",
            "- Adds a penalty equal to the square of the weights multiplied by a regularization strength ($\\lambda_2$).\n",
            "- The L2 regularization term in the loss function is: $$\\lambda_2 \\sum_{i=1}^{n} w_i^2$$\n",
            "- **Effect:** Shrinks the weights towards zero but rarely makes them exactly zero. It helps to reduce the impact of large weights, making the model less sensitive to individual data points.\n",
            "- In this model, the L2 regularization strength ($\\lambda_2$) is set to 0.001.\n",
            "\n",
            "**L1L2 Regularizer in Keras:**\n",
            "- The `regularizers.L1L2(l1=l1_reg, l2=l2_reg)` in Keras applies both L1 and L2 regularization simultaneously to the kernel weights of the dense layers.\n",
            "- The total regularization loss added to the main loss function will be:\n",
            "  $$\\text{Loss}_{regularization} = \\lambda_1 \\sum |w_i| + \\lambda_2 \\sum w_i^2$$\n",
            "- By applying this regularizer to the `kernel_regularizer` argument of the `Dense` layers, we are penalizing large weights in the hidden layers during training.\n",
            "\n",
            "**Visualization:**\n",
            "The `tf.keras.utils.plot_model` function has been used to visualize the architecture of the regularized ANN.\n",
            "The generated image ('regularized_ann_model.png') shows:\n",
            "- The layers of the network (Input, Dense hidden layers, Output).\n",
            "- The shape of the output at each layer.\n",
            "- The name of each layer.\n",
            "- While the visualization shows the structure, it **doesn't directly visualize the effect of regularization** on the weights themselves. Visualizing the weight values would typically involve inspecting the trained model's weights after training on some data, perhaps through histograms or heatmaps.\n",
            "\n",
            "To further visualize the effect of regularization, you would typically:\n",
            "1. Train the model with and without regularization on the same dataset.\n",
            "2. Examine the distribution of the weights in the trained models (e.g., using histograms). You would likely observe that the regularized model has smaller magnitude weights compared to the unregularized model. The L1 regularized model might also show more weights close to zero.\n",
            "3. Compare the performance of the models on a separate test set. A well-regularized model should generalize better and have lower test error than an overfit unregularized model.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the ANN model with L1 and L2 regularization\n",
        "def create_regularized_ann(input_dim, hidden_units, output_dim, l1_reg=0.01, l2_reg=0.01):\n",
        "    \"\"\"\n",
        "    Creates an Artificial Neural Network (ANN) model with L1 and L2 regularization\n",
        "    applied to the kernel weights of the hidden layers.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): The dimensionality of the input features.\n",
        "        hidden_units (list of int): A list specifying the number of units in each hidden layer.\n",
        "        output_dim (int): The dimensionality of the output.\n",
        "        l1_reg (float): The L1 regularization strength (lambda).\n",
        "        l2_reg (float): The L2 regularization strength (lambda).\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(input_dim,)))\n",
        "\n",
        "    # Add hidden layers with L1 and L2 regularization on kernel weights\n",
        "    for units in hidden_units:\n",
        "        model.add(layers.Dense(units, activation='relu',\n",
        "                               kernel_regularizer=regularizers.L1L2(l1=l1_reg, l2=l2_reg)))\n",
        "\n",
        "    # Add the output layer\n",
        "    model.add(layers.Dense(output_dim, activation='sigmoid'))  # Assuming binary classification\n",
        "\n",
        "    return model\n",
        "\n",
        "# Model parameters\n",
        "input_dimension = 10\n",
        "hidden_layer_units = [64, 32]\n",
        "output_dimension = 1\n",
        "l1_strength = 0.005\n",
        "l2_strength = 0.001\n",
        "\n",
        "# Create the regularized ANN model\n",
        "regularized_model = create_regularized_ann(input_dimension, hidden_layer_units,\n",
        "                                           output_dimension, l1_strength, l2_strength)\n",
        "\n",
        "# Visualize the model architecture\n",
        "tf.keras.utils.plot_model(regularized_model, to_file='regularized_ann_model.png',\n",
        "                          show_shapes=True, show_layer_names=True, dpi=96)\n",
        "\n",
        "print(\"Regularized ANN model architecture saved to 'regularized_ann_model.png'\")\n",
        "\n",
        "# --- Explanation and Visualization Details ---\n",
        "\n",
        "print(\"\\n--- Explanation of the Model and Regularization ---\")\n",
        "print(\"\\n**Model Architecture:**\")\n",
        "print(\"The model is a sequential Artificial Neural Network (ANN) built using TensorFlow/Keras.\")\n",
        "print(\"It consists of the following layers:\")\n",
        "print(f\"- **Input Layer:** Accepts input features of dimension {input_dimension}.\")\n",
        "for i, units in enumerate(hidden_layer_units):\n",
        "    print(f\"- **Hidden Layer {i+1}:** A dense layer with {units} units and ReLU activation.\")\n",
        "    print(f\"  - **Kernel Regularization:** Both L1 and L2 regularization are applied to the weights (kernel) of this layer.\")\n",
        "print(f\"- **Output Layer:** A dense layer with {output_dimension} unit and sigmoid activation (for binary classification).\")\n",
        "\n",
        "print(\"\\n**L1 and L2 Regularization:**\")\n",
        "print(\"Regularization techniques are used to prevent overfitting in machine learning models by adding a penalty term to the loss function.\")\n",
        "print(\"This penalty discourages the model from learning overly complex patterns from the training data.\")\n",
        "\n",
        "print(\"\\n**1. L1 Regularization (Lasso):**\")\n",
        "print(\"- Adds a penalty equal to the absolute value of the weights multiplied by a regularization strength ($\\\\lambda_1$).\")\n",
        "print(\"- The L1 regularization term in the loss function is: $$\\\\lambda_1 \\sum_{i=1}^{n} |w_i|$$\")\n",
        "print(\"- **Effect:** Tends to drive some weights to exactly zero, leading to sparse weight matrices. This can be useful for feature selection as it effectively makes some features irrelevant.\")\n",
        "print(f\"- In this model, the L1 regularization strength ($\\\\lambda_1$) is set to {l1_strength}.\")\n",
        "\n",
        "print(\"\\n**2. L2 Regularization (Ridge):**\")\n",
        "print(\"- Adds a penalty equal to the square of the weights multiplied by a regularization strength ($\\\\lambda_2$).\")\n",
        "print(\"- The L2 regularization term in the loss function is: $$\\\\lambda_2 \\sum_{i=1}^{n} w_i^2$$\")\n",
        "print(\"- **Effect:** Shrinks the weights towards zero but rarely makes them exactly zero. It helps to reduce the impact of large weights, making the model less sensitive to individual data points.\")\n",
        "print(f\"- In this model, the L2 regularization strength ($\\\\lambda_2$) is set to {l2_strength}.\")\n",
        "\n",
        "print(\"\\n**L1L2 Regularizer in Keras:**\")\n",
        "print(\"- The `regularizers.L1L2(l1=l1_reg, l2=l2_reg)` in Keras applies both L1 and L2 regularization simultaneously to the kernel weights of the dense layers.\")\n",
        "print(\"- The total regularization loss added to the main loss function will be:\")\n",
        "print(\"  $$\\\\text{Loss}_{regularization} = \\\\lambda_1 \\sum |w_i| + \\\\lambda_2 \\sum w_i^2$$\")\n",
        "print(\"- By applying this regularizer to the `kernel_regularizer` argument of the `Dense` layers, we are penalizing large weights in the hidden layers during training.\")\n",
        "\n",
        "print(\"\\n**Visualization:**\")\n",
        "print(\"The `tf.keras.utils.plot_model` function has been used to visualize the architecture of the regularized ANN.\")\n",
        "print(\"The generated image ('regularized_ann_model.png') shows:\")\n",
        "print(\"- The layers of the network (Input, Dense hidden layers, Output).\")\n",
        "print(\"- The shape of the output at each layer.\")\n",
        "print(\"- The name of each layer.\")\n",
        "print(\"- While the visualization shows the structure, it **doesn't directly visualize the effect of regularization** on the weights themselves. Visualizing the weight values would typically involve inspecting the trained model's weights after training on some data, perhaps through histograms or heatmaps.\")\n",
        "\n",
        "print(\"\\nTo further visualize the effect of regularization, you would typically:\")\n",
        "print(\"1. Train the model with and without regularization on the same dataset.\")\n",
        "print(\"2. Examine the distribution of the weights in the trained models (e.g., using histograms). You would likely observe that the regularized model has smaller magnitude weights compared to the unregularized model. The L1 regularized model might also show more weights close to zero.\")\n",
        "print(\"3. Compare the performance of the models on a separate test set. A well-regularized model should generalize better and have lower test error than an overfit unregularized model.\")"
      ]
    }
  ]
}